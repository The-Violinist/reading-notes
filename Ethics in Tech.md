David Armstrong 12-3-2020

The first article I read was about employee pushback over Google’s Dragonfly search engine. 
Dragonfly was a project aimed at providing China with a search engine which could be censored 
in whatever way deemed necessary by the Chinese government. 1,400 employees signed a letter 
to seek more information and possibly an investigation into the ethical ramifications involved 
in such an undertaking. A precedence was brought up involving the halting of a Pentagon deal
which involved the development of facial recognition software to be installed on drones in order 
to better perform surgical strikes. This work was stopped and Google promised to not use AI tech 
to “harm others”. The correlation between these 2 events is as diverse as comparing apples and, 
oh I don’t know, horses. On the same token, would one argue that Boeing should not do business 
with the air force because they will use the planes to harm others? Should steel manufacturers 
not supply the navy with materials? The only argument that can be made is that one does not trust 
our government to use the technology in an ethical way. If that is the case, then simply state that. 
And Firefly? The Chinese government has had an epic history of human rights abuses from silencing 
of free speech all the way up to re-education camps where they routinely perform organ harvesting 
on “dissenters”. I am not surprised they would like a search engine that censors. Let’s not 
compare that with saving the lives of our troops.

The second article pertained to self driving cars and cybersecurity. How secure are autonomous 
vehicles? With anything that must connect to the internet, a satellite, or any other wireless 
device, there are inherent risks. Ultimately, the degree of risk comes down to the attack surface. 
Hacks have already been documented wherein the brakes could be remotely activated, or a car could 
be remotely hijacked. The DOT recently released guidelines for making cybersecurity a priority in 
the development of autonomous cars. But should there really be a need for regulation? Just from a 
capitalist standpoint, bad publicity is never good for revenue. And that aside, the safety of 
innocent life should be the #1 priority in any situation. From an ethical standpoint, self-driving 
car manufacturers should make sure their priorities are straight. I personally think that there 
should not be any autonomous cars and do not believe they can be made to be entirely safe. If they 
are proven to be safer than human drivers, possibly. But with the added factor of security -- let 
alone the inherent possibility of bugs (I mean, how many times do computers encounter problems?) -- 
I would never trust a car with my life and hope to not be around one when something goes wrong.
